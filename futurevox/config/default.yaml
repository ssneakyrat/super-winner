# Datasets
datasets:
  data_raw: "datasets/gin"

# Audio
audio:
  sample_rate: 22050
  n_fft: 1024
  hop_length: 256
  n_mels: 80
  fmin: 50
  fmax: 1000

# Training configuration
training:
  # Common parameters
  seed: 42
  precision: "bf16-mixed"  # Options: 32, 16, bf16, bf16-mixed
  max_epochs: 10
  gradient_clip_val: 1.0
  accumulate_grad_batches: 8
  val_check_interval: 0.25  # Validate every 25% of an epoch
  log_every_n_steps: 50
  
  # Optimizer
  optimizer:
    name: "AdamW"
    lr: 5.0e-5
    weight_decay: 0.01
    eps: 1.0e-8
    beta1: 0.9
    beta2: 0.999
  
  # Scheduler
  scheduler:
    name: "cosine"
    warmup_steps: 1000
    max_steps: 50000  # Will be recalculated based on data
    num_cycles: 1
    
  # Core model specific training
  core:
    batch_size: 32
    max_length: 1024
    lr: 3.0e-5
    weight_decay: 0.01
    
  # Chat-Instruct module specific training
  chat_instruct:
    batch_size: 16
    max_length: 2048
    lr: 5.0e-5
    weight_decay: 0.01
    freeze_core_layers: True  # Whether to freeze the core model when training
    lr_core_multiplier: 0.1  # If not frozen, use this multiplier for core LR

# Data configuration
data:
  train_path: "data/train/"
  val_path: "data/val/"
  test_path: "data/test/"
  tokenizer_path: "gpt2"  # Use standard GPT-2 tokenizer or path to custom
  max_seq_length: 2048
  num_workers: 4
  prefetch_factor: 2
  shuffle_buffer_size: 10000
  
  # Core model data
  core_data:
    file_pattern: "*.jsonl"
    text_key: "text"

  # Chat-Instruct data
  chat_instruct_data:
    file_pattern: "*.jsonl"
    input_key: "input"
    response_key: "response"
    role_key: "role"
    instruction_key: "instruction_type"

# Checkpoint configuration
checkpoint:
  dirpath: "checkpoints/"
  filename: "{epoch:02d}-{val_loss:.2f}"
  save_top_k: 3
  monitor: "val_loss"
  mode: "min"
  auto_insert_metric_name: False
  every_n_epochs: 1
  save_last: True

# Logging configuration
logging:
  logger: "tensorboard"  # "tensorboard" or "wandb"
  log_dir: "logs/"
  name: "modular-lm"
  version: null  # Auto-increment
  
  # TensorBoard specific
  tensorboard:
    default_hp_metric: True
    log_graph: True
    
  # Weights & Biases specific (if used)
  wandb:
    project: "modular-lm"
    entity: null
    tags: ["modular", "transformer", "chat-instruct"]

# Inference configuration
inference:
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  repetition_penalty: 1.1
  max_new_tokens: 512
  do_sample: True